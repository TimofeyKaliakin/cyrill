{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d0bba78-f438-47d0-ab7b-e8f0970e7346",
   "metadata": {},
   "source": [
    "# Импорты и настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5dcde28-c9e6-43a9-b3c1-c0360848df63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nortlite/mipt/practice/cyrill/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-12-19 11:05:33.102919: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-12-19 11:05:33.132887: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-12-19 11:05:33.742456: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "from preprocessing.augmentation_pipeline import AugmentationPipeline\n",
    "from preprocessing.configs import PipelineConfig\n",
    "from preprocessing.transforms import (\n",
    "    BadPhotoCopyAugmentation,\n",
    "    DilationAugmentation,\n",
    "    ElasticTransformAugmentation,\n",
    "    ErosionAugmentation,\n",
    "    GridDistortionAugmentation,\n",
    "    MotionBlurAugmentation,\n",
    "    ScaleAugmentation,\n",
    "    ScribblesAugmentation,\n",
    "    ShearAugmentation,\n",
    "    WaterMarkAugmentation,\n",
    ")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Sequence, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import (\n",
    "    TrOCRProcessor,\n",
    "    VisionEncoderDecoderModel,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeb5b6d-5977-47bd-af01-fee743bcbd36",
   "metadata": {},
   "source": [
    "# Загрузка датасета и предобработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0658d5d1-c06a-4d3d-8133-88750f062d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset(\"Timka28/cyrillic_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20106c57-cbea-48ac-adb9-05f2126fb7d8",
   "metadata": {},
   "source": [
    "## Настройка пайплайна аугментаций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525f1375-07b0-425c-a820-622706c7742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_config = PipelineConfig(\n",
    "    p_aug=0.5,\n",
    "    augmentations={\n",
    "        'bad_photo': BadPhotoCopyAugmentation(),\n",
    "        'dilation': DilationAugmentation(),\n",
    "        'elastic_transform': ElasticTransformAugmentation(),\n",
    "        'erosion_augmentation': ErosionAugmentation(),\n",
    "        'grid_distortion': GridDistortionAugmentation(),\n",
    "        'motion_blur': MotionBlurAugmentation(),\n",
    "        'scale': ScaleAugmentation(),\n",
    "        'scribbles': ScribblesAugmentation(),\n",
    "    },\n",
    "    return_params=True\n",
    ")\n",
    "\n",
    "aug_pipeline = AugmentationPipeline(config=aug_config, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f968dfb8-5e76-4f2e-8dd6-802b917cd976",
   "metadata": {},
   "source": [
    "# Пайплайн обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7b00c2-ae5d-4e1c-bc5b-0c644db39670",
   "metadata": {},
   "source": [
    "## Конфигурация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bae7f97-732b-4539-9605-733c9d8118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "MODEL_NAME = \"microsoft/trocr-small-handwritten\"\n",
    "DATASET_NAME = \"Timka28/cyrillic_small\"\n",
    "\n",
    "OUTPUT_DIR = \"./trocr_cyrillic_small_ft\"\n",
    "\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "MAX_TARGET_LENGTH = 512\n",
    "GEN_MAX_LENGTH = 512\n",
    "\n",
    "\n",
    "FP16 = True\n",
    "NUM_EPOCHS = 70\n",
    "LR = 5e-5\n",
    "TRAIN_BS = 8\n",
    "EVAL_BS = 8\n",
    "GRAD_ACCUM = 1\n",
    "\n",
    "DATALOADER_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a69377-4269-46f8-b941-c1fa93536b7e",
   "metadata": {},
   "source": [
    "## Утилиты: нормализация + расстояния Левенштейна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb26bebf-c5b6-4d46-aded-2b9c68e2dff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ws_re = re.compile(r\"\\s+\")\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    # минимальная нормализация: убрать лишние пробелы/переводы строк\n",
    "    s = s.replace(\"\\u00A0\", \" \")   # non-breaking space\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    s = _ws_re.sub(\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def levenshtein(seq_a: Union[str, Sequence[Any]], seq_b: Union[str, Sequence[Any]]) -> int:\n",
    "    \"\"\"\n",
    "    Levenshtein distance.\n",
    "    Работает и для строк, и для списков токенов (слов).\n",
    "    \"\"\"\n",
    "    a = seq_a\n",
    "    b = seq_b\n",
    "    if a == b:\n",
    "        return 0\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    if len_a == 0:\n",
    "        return len_b\n",
    "    if len_b == 0:\n",
    "        return len_a\n",
    "\n",
    "    # делаем b короче для экономии памяти\n",
    "    if len_a < len_b:\n",
    "        a, b = b, a\n",
    "        len_a, len_b = len_b, len_a\n",
    "\n",
    "    prev = list(range(len_b + 1))\n",
    "    for i in range(1, len_a + 1):\n",
    "        cur = [i]\n",
    "        ca = a[i - 1]\n",
    "        for j in range(1, len_b + 1):\n",
    "            cb = b[j - 1]\n",
    "            ins = cur[j - 1] + 1\n",
    "            delete = prev[j] + 1\n",
    "            sub = prev[j - 1] + (0 if ca == cb else 1)\n",
    "            cur.append(min(ins, delete, sub))\n",
    "        prev = cur\n",
    "    return prev[-1]\n",
    "\n",
    "def right_metrics(pred_texts: List[str], ref_texts: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    right_symbols: доля правильно распознанных символов (через edit distance по символам)\n",
    "    right_words:   доля правильно распознанных слов (через edit distance по словам)\n",
    "    \"\"\"\n",
    "    total_chars = 0\n",
    "    correct_chars = 0\n",
    "\n",
    "    total_words = 0\n",
    "    correct_words = 0\n",
    "\n",
    "    for p, t in zip(pred_texts, ref_texts):\n",
    "        p = normalize_text(p)\n",
    "        t = normalize_text(t)\n",
    "\n",
    "        # symbols\n",
    "        if len(t) > 0:\n",
    "            d = levenshtein(p, t)\n",
    "            total_chars += len(t)\n",
    "            correct_chars += max(len(t) - d, 0)\n",
    "\n",
    "        # words\n",
    "        t_words = t.split()\n",
    "        p_words = p.split()\n",
    "        if len(t_words) > 0:\n",
    "            d_w = levenshtein(p_words, t_words)\n",
    "            total_words += len(t_words)\n",
    "            correct_words += max(len(t_words) - d_w, 0)\n",
    "\n",
    "    right_symbols = (correct_chars / total_chars) if total_chars > 0 else 0.0\n",
    "    right_words = (correct_words / total_words) if total_words > 0 else 0.0\n",
    "    return {\"right_symbols\": right_symbols, \"right_words\": right_words}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f75b90-e4e7-4537-a7c6-76bec5f59c45",
   "metadata": {},
   "source": [
    "## Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71b3e813-13a6-42b2-b8d8-eda6a7ac8580",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForTrOCR:\n",
    "    processor: TrOCRProcessor\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        pixel_values = torch.stack([f[\"pixel_values\"] for f in features])\n",
    "\n",
    "        labels = [f[\"labels\"] for f in features]\n",
    "        labels_padded = self.processor.tokenizer.pad(\n",
    "            {\"input_ids\": labels},\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        labels_padded[labels_padded == self.processor.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": labels_padded}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7f02c-c74d-4c78-ab6f-a1322b8610e8",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4aaf4eb8-2e4b-4113-83b8-84b127440ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 4425 Test size: 1107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-handwritten and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': 0}.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38780' max='38780' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38780/38780 41:49, Epoch 70/70]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.711000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.459700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.385900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.320000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.122000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>2.979500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>2.905600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>2.852200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>2.773500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>2.724300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>2.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>2.587600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>2.504500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>2.493800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>2.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>2.396300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>2.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>2.280600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>2.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>2.128500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>2.157000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>2.093800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>2.011800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>2.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>1.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>1.940600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>1.896900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>1.874600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>1.871400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>1.894700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>1.819500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>1.823200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>1.799600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FINAL METRICS ===\n",
      "TRAIN: {}\n",
      "TRAIN right_words: 0.008306285554741213\n",
      "TRAIN right_symbols: 0.19018595495733262\n",
      "\n",
      "TEST right_words: 0.0035671819262782403\n",
      "TEST right_symbols: 0.1667991675824818\n",
      "\n",
      "=== DEMO SAMPLE ===\n",
      "GT: репертуаре балетных \n",
      "PR: наедга доы вслх \n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def main():\n",
    "    set_seed(SEED)\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"Device:\", device)\n",
    "\n",
    "    # 1) Load dataset and split 80/20\n",
    "    raw = load_dataset(DATASET_NAME)  # есть только split=train, делаем свой split\n",
    "    ds = raw[\"train\"].train_test_split(test_size=TEST_SIZE, seed=SEED, shuffle=True)\n",
    "\n",
    "    # оставим только нужные колонки\n",
    "    keep_cols = [\"image\", \"text\"]\n",
    "    train_raw = ds[\"train\"].remove_columns([c for c in ds[\"train\"].column_names if c not in keep_cols])\n",
    "    test_raw  = ds[\"test\"].remove_columns([c for c in ds[\"test\"].column_names if c not in keep_cols])\n",
    "\n",
    "    print(\"Train size:\", len(train_raw), \"Test size:\", len(test_raw))\n",
    "\n",
    "    # 2) Load processor + model\n",
    "    processor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    # важные config-поля для VisionEncoderDecoder\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.vocab_size = model.config.decoder.vocab_size\n",
    "    model.config.max_length = GEN_MAX_LENGTH\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # 3) Transforms (on-the-fly)\n",
    "    def transform(batch):\n",
    "        images = []\n",
    "        for idx, img in enumerate(batch[\"image\"]):\n",
    "            # здесь применяем аугментации\n",
    "            img = aug_pipeline(img, idx=idx)[0]\n",
    "            if isinstance(img, Image.Image):\n",
    "                pil = img\n",
    "            else:\n",
    "                pil = Image.fromarray(np.array(img))\n",
    "            images.append(pil.convert(\"RGB\"))\n",
    "\n",
    "        pixel_values = processor(images=images, return_tensors=\"pt\").pixel_values\n",
    "\n",
    "        # labels: токенизация текста\n",
    "        texts = [t if t is not None else \"\" for t in batch[\"text\"]]\n",
    "        tokenized = processor.tokenizer(\n",
    "            texts,\n",
    "            truncation=True,\n",
    "            max_length=MAX_TARGET_LENGTH,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": tokenized[\"input_ids\"],\n",
    "        }\n",
    "\n",
    "    train_ds = train_raw.with_transform(transform)\n",
    "    test_ds = test_raw.with_transform(transform)\n",
    "\n",
    "    data_collator = DataCollatorForTrOCR(processor)\n",
    "\n",
    "    # 4) Metrics for Trainer\n",
    "    def compute_metrics(eval_pred):\n",
    "        preds = eval_pred.predictions\n",
    "        labels = eval_pred.label_ids\n",
    "\n",
    "        # Seq2SeqTrainer иногда возвращает tuple\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        labels = np.where(labels == -100, processor.tokenizer.pad_token_id, labels)\n",
    "\n",
    "        pred_str = processor.batch_decode(preds, skip_special_tokens=True)\n",
    "        label_str = processor.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        m = right_metrics(pred_str, label_str)\n",
    "        return {\n",
    "            \"right_words\": m[\"right_words\"],\n",
    "            \"right_symbols\": m[\"right_symbols\"],\n",
    "        }\n",
    "\n",
    "    # 5) TrainingArguments + Trainer\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        seed=SEED,\n",
    "\n",
    "        learning_rate=LR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "\n",
    "        per_device_train_batch_size=TRAIN_BS,\n",
    "        per_device_eval_batch_size=EVAL_BS,\n",
    "        gradient_accumulation_steps=GRAD_ACCUM,\n",
    "\n",
    "        fp16=FP16,\n",
    "        bf16=False,\n",
    "\n",
    "        predict_with_generate=True,\n",
    "        generation_max_length=GEN_MAX_LENGTH,\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=1000,\n",
    "\n",
    "        save_total_limit=2,\n",
    "\n",
    "        dataloader_num_workers=DATALOADER_WORKERS,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=test_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    # 6) Train (с метриками на test на каждой эпохе)\n",
    "    trainer.train()\n",
    "\n",
    "    # 7) Финальные метрики на train и test (как вы просили)\n",
    "    train_pred = trainer.predict(train_ds, max_length=GEN_MAX_LENGTH)\n",
    "    test_pred  = trainer.predict(test_ds,  max_length=GEN_MAX_LENGTH)\n",
    "\n",
    "    print(\"\\n=== FINAL METRICS ===\")\n",
    "    print(\"TRAIN:\", {k: float(v) for k, v in train_pred.metrics.items() if k.startswith(\"test_\") is False})\n",
    "    # trainer.predict кладёт метрики в dict вида {\"test_right_words\":...} (для любого predict)\n",
    "    # поэтому распечатаем явно:\n",
    "    print(\"TRAIN right_words:\", float(train_pred.metrics.get(\"test_right_words\", 0.0)))\n",
    "    print(\"TRAIN right_symbols:\", float(train_pred.metrics.get(\"test_right_symbols\", 0.0)))\n",
    "\n",
    "    print(\"\\nTEST right_words:\", float(test_pred.metrics.get(\"test_right_words\", 0.0)))\n",
    "    print(\"TEST right_symbols:\", float(test_pred.metrics.get(\"test_right_symbols\", 0.0)))\n",
    "\n",
    "    # 8) Демонстрация распознавания одной картинки\n",
    "    idx = random.randrange(len(test_raw))\n",
    "    sample = test_raw[idx]\n",
    "    image = sample[\"image\"].convert(\"RGB\")\n",
    "    gt_text = sample[\"text\"]\n",
    "\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(inputs.pixel_values, max_length=GEN_MAX_LENGTH)\n",
    "    pred_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    print(\"\\n=== DEMO SAMPLE ===\")\n",
    "    print(\"GT:\", normalize_text(gt_text)[:500], \"...\" if len(gt_text) > 500 else \"\")\n",
    "    print(\"PR:\", normalize_text(pred_text)[:500], \"...\" if len(pred_text) > 500 else \"\")\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure()\n",
    "        plt.imshow(image)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51d46d-c7fc-44ed-81c5-aba8c37e1bc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
